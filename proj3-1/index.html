<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>src</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}

kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
}
</style>

<style type="text/css">
/**
 * prism.js default theme for JavaScript, CSS and HTML
 * Based on dabblet (http://dabblet.com)
 * @author Lea Verou
 */

code[class*="language-"],
pre[class*="language-"] {
	color: black;
	background: none;
	text-shadow: 0 1px white;
	font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
	word-wrap: normal;
	line-height: 1.5;

	-moz-tab-size: 4;
	-o-tab-size: 4;
	tab-size: 4;

	-webkit-hyphens: none;
	-moz-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
}

pre[class*="language-"]::-moz-selection, pre[class*="language-"] ::-moz-selection,
code[class*="language-"]::-moz-selection, code[class*="language-"] ::-moz-selection {
	text-shadow: none;
	background: #b3d4fc;
}

pre[class*="language-"]::selection, pre[class*="language-"] ::selection,
code[class*="language-"]::selection, code[class*="language-"] ::selection {
	text-shadow: none;
	background: #b3d4fc;
}

@media print {
	code[class*="language-"],
	pre[class*="language-"] {
		text-shadow: none;
	}
}

/* Code blocks */
pre[class*="language-"] {
	padding: 1em;
	margin: .5em 0;
	overflow: auto;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
	background: #f5f2f0;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
	padding: .1em;
	border-radius: .3em;
	white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
	color: slategray;
}

.token.punctuation {
	color: #999;
}

.namespace {
	opacity: .7;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
	color: #905;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
	color: #690;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
	color: #a67f59;
	background: hsla(0, 0%, 100%, .5);
}

.token.atrule,
.token.attr-value,
.token.keyword {
	color: #07a;
}

.token.function {
	color: #DD4A68;
}

.token.regex,
.token.important,
.token.variable {
	color: #e90;
}

.token.important,
.token.bold {
	font-weight: bold;
}
.token.italic {
	font-style: italic;
}

.token.entity {
	cursor: help;
}
</style>


</head>

<body>

<h1 id="toc_0">Project 3</h1>

<h2 id="toc_1">Introduction</h2>

<p>In this project we developed a performant, end-to-end ray tracing system capable of rendering 3D scenes through physical simulation of both direct and global lighting. Additionally, we implemented features ranging from bounding volume hierarchies and adaptive sampling all the way up to fully GPU accelerated rendering in an effort to produce highly detailed renders in a reasonable (and often very short!) amount of time.</p>

<p>Our team worked on this project by creating two separate, complete implementations of the project. We did for two reasons. Firstly, we believe that this helps both of us have a much stronger grasp on the material as a lot of the value of this class comes from the process of implementation itself. Secondly, we found this to be a productive means for debugging as having two parallel versions allowed us to more quickly catch and diagnose bugs by swapping in different components across versions until it worked correctly so as to narrow down the source of the issue.</p>

<h2 id="toc_2">Part 1 (Ray Generation and Scene Intersection)</h2>

<p>In this part, we implemented both ray generation and primitive 3D intersection tests for both triangles and spheres. This alone allows us to render small scenes by coloring geometry with an encoded version of its surface normal:</p>

<table>
<thead>
<tr>
<th><code>CBspheres.dae</code> (s=1)</th>
</tr>
</thead>

<tbody>
<tr>
<td><img src="./images/part1and2/CBspheres.png" alt=""></td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th><code>CBcoil.dae</code> (s=1)</th>
</tr>
</thead>

<tbody>
<tr>
<td><img src="./images/part1and2/CBcoil.png" alt=""></td>
</tr>
</tbody>
</table>

<p>To do this, our pipeline begins by casting a variable number of rays (either <code>ns_aa</code> or an adaptive quantity as discussed in part 5) for each pixel. These rays are generated by the camera such that each ray passes through a uniformly random pixel offset through the screen projection plane. The ray then travels out into the world, intersecting with the geometry in the scene.</p>

<p>Our ray tracer supports intersections against both spheres and triangles. We detect sphere intersection by treating the sphere as an implicit surface and solving the quadratic equation for the intersections. To detect triangle intersections, we make use of the Möller Trumbore algorithm. The core of this approach comes from analytically solving for the intersection between a ray and a triangle through a linear system. We do this by first setting the two primitives--namely, the ray and the point of intersection within the triangle (as defined by the barycentrically interpolated position of triangle&#39;s vertices)--and then decomposing its constant and unknowns into a <code>M * x = b</code> form, where <code>M</code> is a 3x3 column vector matrix. We are then able to invert <code>M</code> (either analytically or via the statically known 3x3 determinant) to solve for our unknown vector. This provides us with both <code>t</code> (the time of intersection) and two of the three barycentric coordinates (which, naturally, yields the third), which allows us to fully determine the point of intersection. While this is great, we do also need to detect when we <em>have not</em> intersected with the triangle. We don&#39;t actually need to any additional computation, however, since we can determine this simply by verifying the results of the system and checking that the solution is valid (i.e. <code>t &gt;= 0</code> and all barycentric coordinates are in range). If <em>any</em> condition fails, we know that the point of intersection is not valid and thus that there is no such intersection. This allows us to quickly determine if and where a triangle and a ray have an intersection.</p>

<h2 id="toc_3">Part 2 (Bounding Volume Hierarchy)</h2>

<p>To improve ray intersection performance, we implemented a bounding volume hierarchy. This allows us to rapidly rule out large swathes of geometry without needing to test all of it individually. In an ideal case, this can allow a ray which entirely misses the scene to be ruled out by a single intersection test rather than order-n comparisons against each piece of geometry in the scene. In general, however, this allows a well-formed ray intersecting a balanced-ly divided scene to require only order-log(n) comparisons, which is a phenomenal speedup over the previous exhaustive algorithm, especially given that it comes with no additional signal noise as it is entirely deterministic.</p>

<p>Our team constructs the BVH through a recursive algorithm. This algorithm takes in a linked list of geometry nodes and returns a BVH node which, through some series of twists and turns, covers all the referenced geometry. If the linked list contains fewer than the specified <code>max_leaf_size</code> items, a node containing all of the items is directly constructed. If the linked list contains more than <code>max_leaf_size</code> items, we use an &quot;even split&quot; heuristic which attempts to select the axis which has the nearest to even number of items (as determined by their centroid) on the left and right of the split axis&#39; midpoint. If the split has at least one item on each the left and right set (i.e. forward progress is made), the heuristic&#39;s split is accepted and the geometry is partitioned. If the split <em>does not</em> have at least one item on each side, the split <em>cannot</em> be accepted as it does not make forward progress and thus would lead to infinite recursion if accepted. To resolve these exceptional cases, we arbitrarily partition the set of items in half by their indices. This prevents infinite recursion and exposes further opportunities for these aimlessly split lists to (hopefully) later be properly divided under the heuristic. </p>

<p>We initially chose the &quot;even split&quot; heuristic based on the intuition that the most even split will lead to the nearest division of bounding box volume, and thus shrink the probability that a random ray will intersect the bounding but miss any actual geometry. This works fairly well and it allowed us to render scenes that were previously impractical to render without BVH:</p>

<table>
<thead>
<tr>
<th><code>CBlucy.dae</code> (s=1)</th>
</tr>
</thead>

<tbody>
<tr>
<td><img src="./images/part1and2/CBlucy.png" alt=""></td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th><code>CBbunny.dae</code> (s=1)</th>
</tr>
</thead>

<tbody>
<tr>
<td><img src="./images/part1and2/CBbunny.png" alt=""></td>
</tr>
</tbody>
</table>

<p>To provide better insight into the performance characteristics of the BVH acceleration structure, we perform a comparative analysis between it and the previous brute-force approach in which every piece of geometry is intersected. All tests were performed on the same processor (Apple M1, 8 threads), the same optimization level (<code>-O3</code>), and the same number of samples per pixel (<code>-s 1</code>).</p>

<table>
<thead>
<tr>
<th>Scene</th>
<th>Rendering time (no BVH)</th>
<th>Rendering time (BVH)</th>
<th>Speedup (times)</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>teapot.dae</code></td>
<td>1.8041s</td>
<td>0.0492s</td>
<td>37x</td>
</tr>
<tr>
<td><code>cow.dae</code></td>
<td>4.4483s</td>
<td>0.0387s</td>
<td>115x</td>
</tr>
<tr>
<td><code>CBbunny.dae</code></td>
<td>23.1474s</td>
<td>0.0460s</td>
<td>503x</td>
</tr>
<tr>
<td><code>CBlucy.dae</code></td>
<td>117.9038s</td>
<td>0.0381s</td>
<td>3095x</td>
</tr>
</tbody>
</table>

<p>From these results, we can see that BVH is unilaterially better than the bruteforce approach. BVH yields speedups range from 37 times (a number not to be sneezed at!) all the way up to astronomical 3095 times boosts for certain models. While these speedups are wonderful, the great variance in the speed up bears further analysis. While our team did not deeply investigate why some models performed significantly better than others under BVH, we believe this can be explained by the differing geometry of the various scenes. The two models which saw the largest speedups (<code>CBbunny.dae</code> and <code>CBlucy.dae</code>) have a large number of triangles which take up a relatively small area in the scene. This contrasts with some of the lesser speedups seen with others (such as <code>cow.dae</code>) in which the large number of triangles fill a large portion of the rendered scene. We believe this explains the behavior we see since BVH works by attempting to eliminate large swathes of geometry all at once by throwing away branches which do not intersect the ray. This works very well in sparse scenes (like <code>CBlucy.dae</code>) since most rays will miss all but a handful of triangles and thus only perform a handful of BVH traversals. Conversely, this works slightly less well in dense scenes (like <code>cow.dae</code>) where almost all rays intersect with complex geometry and so a fairly deep traversal is required for significantly more rays than in the aforementioned models.</p>

<h2 id="toc_4">Part 3 (Direct Illumination)</h2>

<p>While coloring surfaces based on their normal yields some visually interesting effects, it&#39;s not quite what we want most of the time. As such, it&#39;s now time to implement some basic, direct illumination!</p>

<p>We did this by first adding support for zero-bounce illumination so that light sources could emit light. With an initial seed of light in the scene, we were then ready to add light bouncing. We did this through two separate means. </p>

<p>First, we implemented hemisphere sampling which, for each camera ray, fired a variable number of light probe rays sampled from a uniform hemisphere on the surface of intersection. Each of these light probe rays returned a light sample which--at this point in the project--ended up just being light from the direct light sources. We then process these samples according to both their angle to the intersected surface and the surface&#39;s BSDF to give both correct coloring and scattering for the object. This allowed us to produce some lit (but noisy!) renderings:</p>

<table>
<thead>
<tr>
<th><code>CBspheres.dae</code> (H, s=64, l=32)</th>
</tr>
</thead>

<tbody>
<tr>
<td><img src="./images/part3/sphere_uniform_sampling.png" alt=""></td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th><code>CBbunny.dae</code> (H, s=64, l=32)</th>
</tr>
</thead>

<tbody>
<tr>
<td><img src="./images/part3/bunny_uniform_sampling.png" alt=""></td>
</tr>
</tbody>
</table>

<p>Next, we implemented light based importance sampling.  This required a slightly different approach: rather than sampling randomly over the hemisphere, we sample against each light. Sampling against a light via the <code>sample_L</code> function gave us both a random direction between our intersection point and the underlying probability that the sample we received was the one returned. With this information in hand, we were then able to cast this new probe ray <em>back</em> towards the light to determine if the light is actually visible with the ray we constructed. If the light is not visible (i.e. we are in shadow!), we do not accumulate any light. If the light is visible, we accumulate the light in a similar way as with the uniform sampler except we use our variable sample probability (PDF) instead of our fixed probability as before. This process yields the following results:</p>

<table>
<thead>
<tr>
<th><code>CBspheres.dae</code> (s=64, l=32)</th>
</tr>
</thead>

<tbody>
<tr>
<td><img src="./images/part3/sphere_light_sampling.png" alt=""></td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th><code>CBbunny.dae</code> (s=64, l=32)</th>
</tr>
</thead>

<tbody>
<tr>
<td><img src="./images/part3/bunny_light_sampling.png" alt=""></td>
</tr>
</tbody>
</table>

<p>To provide further insight into the behavior of importance sampling, we consider an experiment against the <code>CBbunny.dae</code> scene where we focus on the quality of the soft shadows as we manipulate the number of light rays:</p>

<table>
<thead>
<tr>
<th>Light rays</th>
<th>Result (s=1)</th>
</tr>
</thead>

<tbody>
<tr>
<td>1</td>
<td><img src="./images/part3/sphere_light_sampling_l1.png" alt=""></td>
</tr>
<tr>
<td>4</td>
<td><img src="./images/part3/sphere_light_sampling_l4.png" alt=""></td>
</tr>
<tr>
<td>16</td>
<td><img src="./images/part3/sphere_light_sampling_l16.png" alt=""></td>
</tr>
<tr>
<td>64</td>
<td><img src="./images/part3/sphere_light_sampling_l64.png" alt=""></td>
</tr>
</tbody>
</table>

<p>We can immediately make a few key observations. Firstly, in the single light ray example we can see that the shadows are either mostly lit (modulo Lambert&#39;s cosine) or fully dark and become more solidly black (in terms of density, not color!) as we move from the edge of the shadow towards the sphere. Both of these are sensible results with one sample because if the random ray is blocked, the pixel ends up being entirely dark. The random ray has a higher probability of being blocked as we move closer to the sphere because there are fewer and fewer paths to the light as the sphere takes up more space from the ray&#39;s perspective. As the number of rays increase, the shadow smoothness slowly increase and the issue of &quot;binary shadows&quot; disappear. This, again, is to be expected since with more samples, we have a greater chance of approximating the true number of paths from the light to the points and thus are less susceptible to spurious ray blockage.</p>

<p>Finally, we consider the difference in quality between the hemisphere sampling method and the importance sampling method:</p>

<table>
<thead>
<tr>
<th>Hemisphere sampling (s=64, l=32)</th>
<th>Light sampling (s=64, l=32)</th>
</tr>
</thead>

<tbody>
<tr>
<td><img src="./images/part3/sphere_uniform_sampling.png" alt=""></td>
<td><img src="./images/part3/sphere_light_sampling.png" alt=""></td>
</tr>
</tbody>
</table>

<p>Although both of these renders were taken with relatively high pixel and light ray sample counts, the light sampling render is significantly less noisy. This is most apparent when inspecting the back wall. In the hemisphere sampling render, we see a very non-smooth coloring of the wall. In the light sampled render, the corner shadows on the back wall are very smooth and we have essentially no perceptible sample noise. This difference can be directly explained by the different sampling mechanisms because the hemisphere sampler has a much higher probability of sampling a random <em>dark</em> point rather than any point on the surface of a light, which leads to a larger variety of dark and light results being accumulated. The importance sampler avoids these zero contribution samples (which essentially only add noise) by intentionally constructing the PDF to avoid them, which allows it to over come the noise (or, &quot;converge&quot;!) much faster, leading to much cleaner renders with significantly fewer rays.</p>

<h2 id="toc_5">Part 4 (Global Illumination)</h2>

<p>In order to implement global illumination, we had to be able to bounce rays off of other surfaces to get indirect lighting. To get the indirect lighting added into the direct lighting, we had to implement <code>at_least_one_bounce_radiance(const Ray &amp;r, const Intersection &amp;isect)</code> (which would add direct and indirect lighting) and add its return value to the return value of <code>zero_bounce_radiance(const Ray &amp;r, const Intersection &amp;isect)</code> (emitted light) within <code>est_radiance_global_illumination(const Ray &amp;r)</code> to get global illumination. </p>

<p>So now the new function to implement is <code>at_least_one_bounce_radiance()</code>. For this, we had to implement <code>ray.depth = max_ray_depth</code> for each ray. If the ray&#39;s maximum depth reached 0, we can just return a 0 vector for the illumination, since we will not be doing any bounces of light. Otherwise, we can set the illumination vector (<code>L_out</code>) to the direct illumination of the intersection point (output of <code>one_bounce_radiance()</code>), and try to recursively send rays between objects in the scene. Now, to get indirect illumination (illumination from objects in the scene other than the light sources), we used and implemented <code>DiffuseBSDF::sample_f()</code> to give us a negative sample direction from which the indirect illumination will come from and a corresponding pdf. </p>

<p>We need to make sure that if <code>ray.depth</code> is greater than or equal to 1 and we haven&#39;t done any indirect illumination yet, we must take an at least one indirect bounce. Otherwise, we can use Russian Roulette to decide if we should continue with the indirect bounces (continue with probability p = 0.7). We do Russian Roulette because if we did exactly <code>max_ray_depth</code> indirect bounces for each ray, we would get a biased result, and Russian Roulette helps probabilistically balance the ray contributions. </p>

<p>If we continue onto the next indirect bounce (after passing the previous conditions), we must create a new ray in the sample direction with a decremented depth (to cap the number of indirect bounces). We will recursively call <code>at_least_one_bounce_radiance()</code> with the newly created ray and weight it by the cosine of the angle of the ray with respect to the surface normal, with the surface BSDF function, and divide it by the sample&#39;s pdf and the Russian Roulette probability (p = 0.7). We can add this value into our <code>L_out</code> and return that. The recursive calls will accumulate all the illuminations from indirect bounces back to our original location in the scene, and that is what we return.</p>

<h3 id="toc_6">Global Illumination Images:</h3>

<p>1024 Samples Per Pixel, 16 Light Samples, Max Depth of 5
|       |  |
| ----------- | ----------- |
| <img src="./images/part4/spheres_global.png" alt="`CBspheres_lambertian`"> <code>CBspheres_lambertian</code>      | <img src="./images/part4/bunny_global.png" alt="`CBbunny`"><code>CBbunny</code>      |
| <img src="./images/part4/dragon_global.png" alt="`dragon`"> <code>dragon</code>      | <img src="./images/part4/wall-e_global.png" alt="`wall-e`"> <code>wall-e</code>      | </p>

<h3 id="toc_7">Direct Only vs Indirect Only Illumination <code>CBbunny.dae</code>:</h3>

<p>1024 Samples Per Pixel, 16 Light Samples, Max Depth of 5
|       |  |
| ----------- | ----------- |
| <img src="./images/part4/bunny_direct_only.png" alt="Direct Illumination Only">Direct Illumination Only      | <img src="./images/part4/bunny_indirect_only.png" alt="Indirect Illumination Only">Indirect Illumination Only       |</p>

<h3 id="toc_8">Different Max Ray Depths <code>CBbunny.dae</code>:</h3>

<p>1024 Samples Per Pixel, 16 Light Samples
|       |  |
| ----------- | ----------- |
| <img src="./images/part4/bunny_m0.png" alt="`CBbunny.dae` Max Ray Depth of 0"> <code>CBbunny.dae</code> Max Ray Depth of 0      | <img src="./images/part4/bunny_m1.png" alt="`CBbunny.dae` Max Ray Depth of 1"> <code>CBbunny.dae</code> Max Ray Depth of 1       |
| <img src="./images/part4/bunny_m2.png" alt="`CBbunny.dae` Max Ray Depth of 2"> <code>CBbunny.dae</code> Max Ray Depth of 2      | <img src="./images/part4/bunny_m3.png" alt="`CBbunny.dae` Max Ray Depth of 3"> <code>CBbunny.dae</code> Max Ray Depth of 3       |
| <img src="./images/part4/bunny_m100.png" alt="`CBbunny.dae` Max Ray Depth of 100"> <code>CBbunny.dae</code> Max Ray Depth of 100      |</p>

<h3 id="toc_9">Different Sample-Per-Pixel Rates <code>CBspheres_lambertian.dae</code>:</h3>

<p>4 Light Samples, Max Depth of 5
|       |  |
| ----------- | ----------- |
| <img src="./images/part4/sphere_s1.png" alt="`CBspheres_lambertian.dae` 1 Sample Per Pixel"> <code>CBspheres_lambertian.dae</code> 1 Sample Per Pixel      | <img src="./images/part4/sphere_s2.png" alt="`CBspheres_lambertian.dae` 2 Sample Per Pixel"> <code>CBspheres_lambertian.dae</code> 2 Sample Per Pixel       |
| <img src="./images/part4/sphere_s4.png" alt="`CBspheres_lambertian.dae` 4 Sample Per Pixel"> <code>CBspheres_lambertian.dae</code> 4 Sample Per Pixel      | <img src="./images/part4/sphere_s8.png" alt="`CBspheres_lambertian.dae` 8 Sample Per Pixel"> <code>CBspheres_lambertian.dae</code> 8 Sample Per Pixel       |
| <img src="./images/part4/sphere_s16.png" alt="`CBspheres_lambertian.dae` 16 Sample Per Pixel"> <code>CBspheres_lambertian.dae</code> 16 Sample Per Pixel      | <img src="./images/part4/sphere_s64.png" alt="`CBspheres_lambertian.dae` 64 Sample Per Pixel"> <code>CBspheres_lambertian.dae</code> 64 Sample Per Pixel      |
|  <img src="./images/part4/sphere_s1024.png" alt="`CBspheres_lambertian.dae` 1024 Sample Per Pixel"> <code>CBspheres_lambertian.dae</code> 1024 Sample Per Pixel      | </p>

<h2 id="toc_10">Part 5 (Adaptive Sampling)</h2>

<p>We use a statistical calculated quantity <code>I = 1.96 * sqrt(variance / n)</code> to determince convergence of pixels, where <code>n</code> is the number of samples and <code>variance</code> is of the <code>n</code> samples. We stop taking more samples for a pixel if <code>I &lt; maxTolerance * mean</code>, where <code>mean</code> is the average of the <code>n</code> samples. Since calculating these statistics at each step is quite expensive, we calculate it every <code>samplesPerBatch</code> samples for each pixel. However, we need to keep a running sum of each sample&#39;s illuminance for calculating the mean (given by <code>illum()</code>), and illuminance squared (for calculating <code>variance</code>). Once our sample number is a multiple of <code>samplesPerBatch</code>, we can calculate the statistics completely and get the <code>mean</code> to be the running sum of illuminance divided by the number of samples done so far (<code>n</code>). We can calculate the variance by this equation: <code>1/(n-1) * (sum of illuminance - (sum of squares of illuminance)/n)</code>. From the mean and variance we can calculate <code>I</code> and compare it to <code>maxTolerance * mean</code>. If it less than the tolerance, we can say we have converged and there is no need to continue taking samples for that pixel, and we can just stop taking samples for that pixel. This will save a lot of computation by preventing us from doing unnecessary calculations that have no discernable impact on the final image.</p>

<h3 id="toc_11">Adaptive Sampling <code>CBbunny.dae</code>:</h3>

<p>2048 Samples Per Pixel, 1 Light Sample, Max Depth of 5, 64 Samples Per Batch, Max Tolerance of 0.05
|       |  |
| ----------- | ----------- |
| <img src="./images/part5/bunny.png" alt="`CBbunny.dae` Adaptive Sampling"> <code>CBbunny.dae</code> Adaptive Sampling      | <img src="./images/part5/bunny_rate.png" alt="`CBbunny.dae` Adaptive Sampling Rate"> <code>CBbunny.dae</code> Adaptive Sampling Rate       |</p>

<h2 id="toc_12">Extra credit: GPU Accelerated Ray Tracing</h2>

<p>We chose to extend this project in a very fun and intellectually rewarding way by adding experimental support for <strong>end-to-end GPU offloading</strong>.</p>

<p>While neither of our team members were particularly familiar with GPUs or how to write compute shaders, we felt this extra credit option provided the perfect opportunity to dive in and see what we could do. Since we were working on macOS devices with Apple GPUs, our best (and, I suppose, only!) option was to make use of Apple&#39;s Metal graphics API. </p>

<p>After a bit of planning and code base spelunking, however, we found that the existing project structure was not very amenable to GPU acceleration. This is primarily because the existing project code works in a very sequential fashion. That is, a single ray is cast and fully evaluated before being rendered out into a single pixel in the framebuffer. While this works just fine for CPUs, this is very problematic for acceleration with GPUs because GPUs work by performing the same function against an enormous pool of data in parallel.</p>

<p>Working around this issue was quite difficult. Initially, we considered trying to insert operation buffering/deferral (i.e. a ray is generated but rather than being immediately traced, it is pushed to a queue which is evaluated in bulk once a certain number of rays arrived) but we found that this pattern was rather difficult to implement efficiently in C++ as it required storing an enormous amount of state (effectively the entire callstack!) for each ray in order to be able to resume execution once the ray had been traced.</p>

<p>After spinning on this issue for a few days, we eventually decided to throw out all the starter code and start (mostly) fresh. Rather than trying to flatten a single part of the pipeline and parallelize it with the GPU, we decided that we would simply do <em>everything</em> from ray generation to frame buffer writing on the GPU! This was an ambitious goal and, therefore, to manage complexity and avoid turning this extra credit into an even bigger time monster, we chose to limit the scope of our acceleration to only tasks 1 and 2 (i.e. BVH accelerated tracing with normals rather than lighting for coloring).</p>

<p>While it took multiple days and dozens of iterations and re-writes, we eventually reached a working pipeline. Our implementation had three main components which we will now explore in detail.</p>

<h3 id="toc_13">µBVH</h3>

<p>To support fast tracing on the GPU, we knew we needed to support BVH based tracing on the GPU. Unfortunately, the representation that we used in the CPU based tracer (<code>bvh.cpp</code>) was not suitable for use on the GPU both because it used lots of external pointers (which are not portable across the CPU-GPU gap!) and because it required an enormous amount of the C++ API, making it entirely unusuable in an embedded environment like a shader core. </p>

<p>Thus, to have a functional BVH on the GPU, we chose to construct a brand, flattened BVH structure which we dubbed the &quot;micro-BVH&quot;. </p>

<p><img src="./images/gpu/gpu1.jpeg" alt=""></p>

<p>This flattened representation was, essentially, the same tree hierarchy and C++ objects as used throughout the CPU variant of the project encoded into a contiguous memory buffer using C structures and buffer offsets. BVH intersection would begin at index zero of the structure and both the left and right bounding boxes would be tested. If an intersection is found for, say, the right child, the index can be taken to find the node <em>or</em> series of triangles and spheres corresponding to that bounding box. This simpler, more compact representation allows both for a much more portable (and thus GPU compatible!) BVH algorithm as well as a much <em>faster</em> traversal since less time is wasted on overhead and pointer chasing.</p>

<p>We provide more detail on how exactly this structure is used in the shader section.</p>

<h3 id="toc_14">GPU Tracer Shim</h3>

<p>Although we were throwing out most of the project code, we didn&#39;t want to do <em>everything</em> from scratch. Thus, we added a new class to the project called <code>GPUTracer</code> which provides a way for the <code>raytrace_renderer</code> to trigger a render using the GPU. This was done so that we can reuse the model parsing functionality as well as a lot of the nice statistics features provided by the project.</p>

<p>In this class, we handle a lot of the Metal boilerplate. Here we create GPU handles, compile shaders, allocate cross-device buffers, issue the correct ray tracing requests, and shuttle data to and from the GPU. This part of the project was not terribly design heavy but it had a fairly steep learning curve as neither of us had used the Metal API, let alone the C++ bindings for that API (the Metal API, as an Apple framework, is actually designed for Objective-C), and so a lot of reading was required to successfully communicate with the GPU.</p>

<h3 id="toc_15">gTracer</h3>

<p>The final, and ostensibly most exciting, piece of the puzzle is the GPU shader program. Our shader program is designed to be executed once per ray, per pixel. That is, it traces a single ray and super samples that ray into the frame buffer. This is achieved in three main phases.</p>

<p>Firstly, the shader generates a uniformly random ray which passes through the pixel corresponding to the pixel using the same technique used in <code>Camera::generate_ray</code>. We actually didn&#39;t initially generate rays as part of our shader program and instead opted to leave ray generation on the CPU. We did this because we thought that ray generation was not terribly expensive and that the convience was worth whatever minimal performance hit we took. This turned out to be <em>seriously wrong</em> for a number of reasons. Most critically, we found that ray generation is actually very time intensive for the CPU and can, in fact, dwarf GPU render time as the ray count reaches into the hundreds of millions. Additionally, storing millions upon millions of rays in memory was problematic for another more practical reason because it is both an enormous waste of space and, as we later discovered, that the Metal API is unable to map buffers larger thatn 4GBs to a GPU. This meant that we not only should be generating rays on the GPU but rather that we <em>needed</em> to generate rays on the GPU if we wanted to support very high sample rates.</p>

<p>Next, with the ray in hand, the shader traverse the BVH tree in a manner very similar to our CPU algorithm. Notably, however, we do not do this recursively both for performance reasons and because Metal does not support recursion for diverge reasons, and so instead opted to use an in-memory operation stack to handle cases where we need to traverse both sides of a node. Intersection tests work almost identically here as they did on the CPU and, in fact, we were able to directly copy and paste most of our algorithms due to matching APIs in Metal and CGL. The tracing process works as expected and eventually yields both a <code>t</code> value and a surface normal.</p>

<p>Once tracing is done, we are able to compute the corresponding pixel color for the surface normal and write it to the frame buffer. Due to the highly parallel nature of GPUs, however, we needed to be careful to make these writes atomic and, due to the Metal API not supporting atomic operations with floating point arguments, we re-encode pixel values as fixed point unsigned integers before writing them out to the frame buffer. </p>

<p>This concludes a single execution of the shader. By executing this procedure against every pixel with a number of iterations, we are able to concurrently cast, trace, and render the result on the GPU!</p>

<h3 id="toc_16">Results</h3>

<p>With this long journey complete, let&#39;s now consider the speed up that all this work provided. We do this by comparing the render time between the CPU variant of our project after task 2 against our GPU accelerated variant. All code in this test was compiled on <code>-O3</code>, all CPU tests ran with 8 threads, and all renders target 2048 supersampled rays per pixel. The results are summarized below:</p>

<table>
<thead>
<tr>
<th>Model</th>
<th>CPU render time</th>
<th>GPU render time</th>
<th>Speedup</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>CBbunny.dae</code></td>
<td>83.4393s</td>
<td>8.929545s</td>
<td>9.34x</td>
</tr>
<tr>
<td><code>CBlucy.dae</code></td>
<td>51.4526s</td>
<td>5.166439s</td>
<td>9.95x</td>
</tr>
<tr>
<td><code>cow.dae</code></td>
<td>61.5785s</td>
<td>3.103047s</td>
<td>19.84x</td>
</tr>
<tr>
<td><code>CBcoil.dae</code></td>
<td>64.1523s</td>
<td>1.235419s</td>
<td>51.92x</td>
</tr>
</tbody>
</table>

<p>As desired, this shows a substantial speedup in the render time for <em>all</em> models. This is not entirely surprising given that this is a task that GPUs are very well suited for given that GPUs work exceedingly well at this task given their enormous number of threads and their much wider floating point pipelines. </p>

<p>We do, however, expect that we could push the GPU <em>much</em> faster with more optimized code and, as we discovered through profiling, a more cache friendly BVH design. We see this most clearly in the huge gap between <code>CBbunny</code> and <code>CBcoil</code> where the smaller and less demanding BVH structure of the coil allowed the GPU to absolutely crush the CPU and render over 50x faster.</p>

<p>Regardless, we are quite satisfied with our first adventure into the art of GPU programming and we hope to apply these skills in our future projects to squeeze out as much extra compute performance as we can.</p>



<script type="text/javascript">
var _self="undefined"!=typeof window?window:"undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?self:{},Prism=function(){var e=/\blang(?:uage)?-(\w+)\b/i,t=0,n=_self.Prism={util:{encode:function(e){return e instanceof a?new a(e.type,n.util.encode(e.content),e.alias):"Array"===n.util.type(e)?e.map(n.util.encode):e.replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/\u00a0/g," ")},type:function(e){return Object.prototype.toString.call(e).match(/\[object (\w+)\]/)[1]},objId:function(e){return e.__id||Object.defineProperty(e,"__id",{value:++t}),e.__id},clone:function(e){var t=n.util.type(e);switch(t){case"Object":var a={};for(var r in e)e.hasOwnProperty(r)&&(a[r]=n.util.clone(e[r]));return a;case"Array":return e.map&&e.map(function(e){return n.util.clone(e)})}return e}},languages:{extend:function(e,t){var a=n.util.clone(n.languages[e]);for(var r in t)a[r]=t[r];return a},insertBefore:function(e,t,a,r){r=r||n.languages;var l=r[e];if(2==arguments.length){a=arguments[1];for(var i in a)a.hasOwnProperty(i)&&(l[i]=a[i]);return l}var o={};for(var s in l)if(l.hasOwnProperty(s)){if(s==t)for(var i in a)a.hasOwnProperty(i)&&(o[i]=a[i]);o[s]=l[s]}return n.languages.DFS(n.languages,function(t,n){n===r[e]&&t!=e&&(this[t]=o)}),r[e]=o},DFS:function(e,t,a,r){r=r||{};for(var l in e)e.hasOwnProperty(l)&&(t.call(e,l,e[l],a||l),"Object"!==n.util.type(e[l])||r[n.util.objId(e[l])]?"Array"!==n.util.type(e[l])||r[n.util.objId(e[l])]||(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,l,r)):(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,null,r)))}},plugins:{},highlightAll:function(e,t){var a={callback:t,selector:'code[class*="language-"], [class*="language-"] code, code[class*="lang-"], [class*="lang-"] code'};n.hooks.run("before-highlightall",a);for(var r,l=a.elements||document.querySelectorAll(a.selector),i=0;r=l[i++];)n.highlightElement(r,e===!0,a.callback)},highlightElement:function(t,a,r){for(var l,i,o=t;o&&!e.test(o.className);)o=o.parentNode;o&&(l=(o.className.match(e)||[,""])[1],i=n.languages[l]),t.className=t.className.replace(e,"").replace(/\s+/g," ")+" language-"+l,o=t.parentNode,/pre/i.test(o.nodeName)&&(o.className=o.className.replace(e,"").replace(/\s+/g," ")+" language-"+l);var s=t.textContent,u={element:t,language:l,grammar:i,code:s};if(!s||!i)return n.hooks.run("complete",u),void 0;if(n.hooks.run("before-highlight",u),a&&_self.Worker){var c=new Worker(n.filename);c.onmessage=function(e){u.highlightedCode=e.data,n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(u.element),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},c.postMessage(JSON.stringify({language:u.language,code:u.code,immediateClose:!0}))}else u.highlightedCode=n.highlight(u.code,u.grammar,u.language),n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(t),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},highlight:function(e,t,r){var l=n.tokenize(e,t);return a.stringify(n.util.encode(l),r)},tokenize:function(e,t){var a=n.Token,r=[e],l=t.rest;if(l){for(var i in l)t[i]=l[i];delete t.rest}e:for(var i in t)if(t.hasOwnProperty(i)&&t[i]){var o=t[i];o="Array"===n.util.type(o)?o:[o];for(var s=0;s<o.length;++s){var u=o[s],c=u.inside,g=!!u.lookbehind,h=!!u.greedy,f=0,d=u.alias;u=u.pattern||u;for(var p=0;p<r.length;p++){var m=r[p];if(r.length>e.length)break e;if(!(m instanceof a)){u.lastIndex=0;var y=u.exec(m),v=1;if(!y&&h&&p!=r.length-1){var b=r[p+1].matchedStr||r[p+1],k=m+b;if(p<r.length-2&&(k+=r[p+2].matchedStr||r[p+2]),u.lastIndex=0,y=u.exec(k),!y)continue;var w=y.index+(g?y[1].length:0);if(w>=m.length)continue;var _=y.index+y[0].length,P=m.length+b.length;if(v=3,P>=_){if(r[p+1].greedy)continue;v=2,k=k.slice(0,P)}m=k}if(y){g&&(f=y[1].length);var w=y.index+f,y=y[0].slice(f),_=w+y.length,S=m.slice(0,w),O=m.slice(_),j=[p,v];S&&j.push(S);var A=new a(i,c?n.tokenize(y,c):y,d,y,h);j.push(A),O&&j.push(O),Array.prototype.splice.apply(r,j)}}}}}return r},hooks:{all:{},add:function(e,t){var a=n.hooks.all;a[e]=a[e]||[],a[e].push(t)},run:function(e,t){var a=n.hooks.all[e];if(a&&a.length)for(var r,l=0;r=a[l++];)r(t)}}},a=n.Token=function(e,t,n,a,r){this.type=e,this.content=t,this.alias=n,this.matchedStr=a||null,this.greedy=!!r};if(a.stringify=function(e,t,r){if("string"==typeof e)return e;if("Array"===n.util.type(e))return e.map(function(n){return a.stringify(n,t,e)}).join("");var l={type:e.type,content:a.stringify(e.content,t,r),tag:"span",classes:["token",e.type],attributes:{},language:t,parent:r};if("comment"==l.type&&(l.attributes.spellcheck="true"),e.alias){var i="Array"===n.util.type(e.alias)?e.alias:[e.alias];Array.prototype.push.apply(l.classes,i)}n.hooks.run("wrap",l);var o="";for(var s in l.attributes)o+=(o?" ":"")+s+'="'+(l.attributes[s]||"")+'"';return"<"+l.tag+' class="'+l.classes.join(" ")+'" '+o+">"+l.content+"</"+l.tag+">"},!_self.document)return _self.addEventListener?(_self.addEventListener("message",function(e){var t=JSON.parse(e.data),a=t.language,r=t.code,l=t.immediateClose;_self.postMessage(n.highlight(r,n.languages[a],a)),l&&_self.close()},!1),_self.Prism):_self.Prism;var r=document.currentScript||[].slice.call(document.getElementsByTagName("script")).pop();return r&&(n.filename=r.src,document.addEventListener&&!r.hasAttribute("data-manual")&&document.addEventListener("DOMContentLoaded",n.highlightAll)),_self.Prism}();"undefined"!=typeof module&&module.exports&&(module.exports=Prism),"undefined"!=typeof global&&(global.Prism=Prism);
</script>

<script type="text/x-mathjax-config">
(function () {

MathJax.Hub.Config({
	'showProcessingMessages': false,
	'messageStyle': 'none'
});

if (typeof MathJaxListener !== 'undefined') {
	MathJax.Hub.Register.StartupHook('End', function () {
		MathJaxListener.invokeCallbackForKey_('End');
	});
}

})();
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


</body>

</html>
